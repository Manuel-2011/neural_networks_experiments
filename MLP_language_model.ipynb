{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data: 32033\n",
      "['emma\\n', 'olivia\\n', 'ava\\n', 'isabella\\n', 'sophia\\n', 'charlotte\\n', 'mia\\n', 'amelia\\n', 'harper\\n', 'evelyn\\n']\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "\n",
    "with(open('names.txt')) as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "print(f'Total data: {len(data)}')\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jaiceion' 'avari' 'shahbaz' 'mehnaz' 'alexzandra' 'harlem' 'naelle'\n",
      " 'cayson' 'rosalea' 'jaselle']\n"
     ]
    }
   ],
   "source": [
    "# Preprocess of the data\n",
    "import numpy as np\n",
    "\n",
    "data = [d.strip().lower() for d in data]\n",
    "data = np.array(data)\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.seed(45)\n",
    "np.random.shuffle(data)\n",
    "\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# Create the tokenizer\n",
    "vocabulary = sorted(list(set(''.join(data))))\n",
    "# add special token\n",
    "S_TOK = '.'\n",
    "ixtos = [S_TOK] + vocabulary\n",
    "print(ixtos)\n",
    "print(len(ixtos))\n",
    "stoix = {s: i for i, s in enumerate(ixtos)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['j', 'a', 'i', 'c', 'e', 'i', 'o', 'n']"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the data\n",
    "tokenized = [[stoix[c] for c in n] for n in data]\n",
    "\n",
    "# Verify the tokenization is working properly\n",
    "[ixtos[t] for t in tokenized[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228146, 228146)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the dataset with a chunk of characters as a context and the next character as label\n",
    "context = 4 # Characters of context\n",
    "\n",
    "def build_dataset(data: list):\n",
    "    X, y = [], []\n",
    "    for name in data:\n",
    "        # Pad word and add final special token\n",
    "        name = [stoix[S_TOK]]*context + name + [stoix[S_TOK]]\n",
    "        if len(name) < context + 1:\n",
    "            print(f'name {name} not large enough')\n",
    "            continue\n",
    "        for i in range(len(name)-context):\n",
    "            ctxt = name[i:i+context]\n",
    "            label = name[i+context]\n",
    "            # print(ctxt, label)\n",
    "            X.append(ctxt)\n",
    "            y.append(label)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = build_dataset(tokenized)\n",
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182516, 22814, 22816)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# Split the data intro training, validation, and test\n",
    "train_samples = int(len(X)*0.8)\n",
    "valid_samples = int(len(X)*0.1)\n",
    "\n",
    "X_train, y_train = X[:train_samples], y[:train_samples]\n",
    "X_valid, y_valid = X[train_samples:train_samples+valid_samples], y[train_samples:train_samples+valid_samples]\n",
    "X_test, y_test = X[train_samples+valid_samples:], y[train_samples+valid_samples:]\n",
    "\n",
    "X_train = torch.tensor(X_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "X_valid = torch.tensor(X_valid)\n",
    "y_valid = torch.tensor(y_valid)\n",
    "X_test = torch.tensor(X_test)\n",
    "y_test = torch.tensor(y_test)\n",
    "\n",
    "len(X_train), len(X_valid), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', '.', '.', '.'] j\n",
      "['.', '.', '.', 'j'] a\n",
      "['.', '.', 'j', 'a'] i\n",
      "['.', 'j', 'a', 'i'] c\n",
      "['j', 'a', 'i', 'c'] e\n",
      "['a', 'i', 'c', 'e'] i\n",
      "['i', 'c', 'e', 'i'] o\n",
      "['c', 'e', 'i', 'o'] n\n",
      "['e', 'i', 'o', 'n'] .\n",
      "['.', '.', '.', '.'] a\n",
      "['.', '.', '.', 'a'] v\n",
      "['.', '.', 'a', 'v'] a\n",
      "['.', 'a', 'v', 'a'] r\n",
      "['a', 'v', 'a', 'r'] i\n",
      "['v', 'a', 'r', 'i'] .\n",
      "['.', '.', '.', '.'] s\n",
      "['.', '.', '.', 's'] h\n",
      "['.', '.', 's', 'h'] a\n",
      "['.', 's', 'h', 'a'] h\n",
      "['s', 'h', 'a', 'h'] b\n",
      "['h', 'a', 'h', 'b'] a\n",
      "['a', 'h', 'b', 'a'] z\n",
      "['h', 'b', 'a', 'z'] .\n",
      "['.', '.', '.', '.'] m\n",
      "['.', '.', '.', 'm'] e\n",
      "['.', '.', 'm', 'e'] h\n",
      "['.', 'm', 'e', 'h'] n\n",
      "['m', 'e', 'h', 'n'] a\n",
      "['e', 'h', 'n', 'a'] z\n",
      "['h', 'n', 'a', 'z'] .\n"
     ]
    }
   ],
   "source": [
    "for d, k in zip(X_train[:30], y[:30]):\n",
    "    print([ixtos[i] for i in d], ixtos[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(2147483647)\n",
    "\n",
    "# Embeddings table\n",
    "emb_size = 4\n",
    "E = torch.randn((len(stoix), emb_size), generator=g, requires_grad=True)\n",
    "\n",
    "# Dense layer\n",
    "W1 = torch.randn((context * emb_size, 100), generator=g, requires_grad=True)\n",
    "b1 = torch.randn((1, 100), generator=g, requires_grad=True)\n",
    "\n",
    "# Final dense layer\n",
    "W2 = torch.randn((100, E.shape[0]), generator=g, requires_grad=True)\n",
    "b2 = torch.randn((1, E.shape[0]), generator=g, requires_grad=True)\n",
    "\n",
    "# Model\n",
    "# emb_layer = E[X_train].view(-1, context * emb_size) # embed each token and concatenate the context tokens\n",
    "# print(emb_layer.shape)\n",
    "# dense_layer = ((emb_layer @ W1) + b1).relu()\n",
    "# print(dense_layer.shape)\n",
    "# dense_layer = (dense_layer @ W2) + b2\n",
    "# print(dense_layer.shape)\n",
    "# logits = dense_layer.exp()\n",
    "# norm = logits.sum(1, keepdim=True)\n",
    "# print(norm.shape)\n",
    "# probs = logits / norm\n",
    "# print(probs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4535"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = [E, W1, b1, W2, b2]\n",
    "\n",
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 2.43256 - Validation loss: 2.44857\n",
      "Epoch 1 - Training loss: 2.39487 - Validation loss: 2.41644\n",
      "Epoch 2 - Training loss: 2.37408 - Validation loss: 2.39407\n",
      "Epoch 3 - Training loss: 2.35881 - Validation loss: 2.37261\n",
      "Epoch 4 - Training loss: 2.34604 - Validation loss: 2.35661\n",
      "Epoch 5 - Training loss: 2.33505 - Validation loss: 2.34553\n",
      "Epoch 6 - Training loss: 2.32555 - Validation loss: 2.33547\n",
      "Epoch 7 - Training loss: 2.31689 - Validation loss: 2.32867\n",
      "Epoch 8 - Training loss: 2.30944 - Validation loss: 2.31785\n",
      "Epoch 9 - Training loss: 2.30302 - Validation loss: 2.31489\n",
      "Epoch 10 - Training loss: 2.29728 - Validation loss: 2.30782\n",
      "Epoch 11 - Training loss: 2.29187 - Validation loss: 2.30418\n",
      "Epoch 12 - Training loss: 2.28703 - Validation loss: 2.29947\n",
      "Epoch 13 - Training loss: 2.28271 - Validation loss: 2.29394\n",
      "Epoch 14 - Training loss: 2.27870 - Validation loss: 2.28917\n",
      "Epoch 15 - Training loss: 2.27486 - Validation loss: 2.28541\n",
      "Epoch 16 - Training loss: 2.27165 - Validation loss: 2.28214\n",
      "Epoch 17 - Training loss: 2.26866 - Validation loss: 2.27947\n",
      "Epoch 18 - Training loss: 2.26581 - Validation loss: 2.27560\n",
      "Epoch 19 - Training loss: 2.26317 - Validation loss: 2.27174\n",
      "Epoch 20 - Training loss: 2.26049 - Validation loss: 2.27106\n",
      "Epoch 21 - Training loss: 2.25813 - Validation loss: 2.26666\n",
      "Epoch 22 - Training loss: 2.25577 - Validation loss: 2.26459\n",
      "Epoch 23 - Training loss: 2.25360 - Validation loss: 2.26267\n",
      "Epoch 24 - Training loss: 2.25152 - Validation loss: 2.26189\n",
      "Epoch 25 - Training loss: 2.24961 - Validation loss: 2.25993\n",
      "Epoch 26 - Training loss: 2.24768 - Validation loss: 2.25905\n",
      "Epoch 27 - Training loss: 2.24600 - Validation loss: 2.25672\n",
      "Epoch 28 - Training loss: 2.24446 - Validation loss: 2.25529\n",
      "Epoch 29 - Training loss: 2.24294 - Validation loss: 2.25370\n",
      "Epoch 30 - Training loss: 2.24152 - Validation loss: 2.25305\n",
      "Epoch 31 - Training loss: 2.24024 - Validation loss: 2.25140\n",
      "Epoch 32 - Training loss: 2.23876 - Validation loss: 2.25164\n",
      "Epoch 33 - Training loss: 2.23750 - Validation loss: 2.24952\n",
      "Epoch 34 - Training loss: 2.23621 - Validation loss: 2.24932\n",
      "Epoch 35 - Training loss: 2.23500 - Validation loss: 2.24771\n",
      "Epoch 36 - Training loss: 2.23394 - Validation loss: 2.24772\n",
      "Epoch 37 - Training loss: 2.23280 - Validation loss: 2.24619\n",
      "Epoch 38 - Training loss: 2.23162 - Validation loss: 2.24656\n",
      "Epoch 39 - Training loss: 2.23036 - Validation loss: 2.24318\n",
      "Epoch 40 - Training loss: 2.22922 - Validation loss: 2.24297\n",
      "Epoch 41 - Training loss: 2.22796 - Validation loss: 2.24178\n",
      "Epoch 42 - Training loss: 2.22691 - Validation loss: 2.24088\n",
      "Epoch 43 - Training loss: 2.22586 - Validation loss: 2.23986\n",
      "Epoch 44 - Training loss: 2.22492 - Validation loss: 2.23876\n",
      "Epoch 45 - Training loss: 2.22390 - Validation loss: 2.23815\n",
      "Epoch 46 - Training loss: 2.22292 - Validation loss: 2.23783\n",
      "Epoch 47 - Training loss: 2.22193 - Validation loss: 2.23576\n",
      "Epoch 48 - Training loss: 2.22102 - Validation loss: 2.23502\n",
      "Epoch 49 - Training loss: 2.22017 - Validation loss: 2.23523\n",
      "Epoch 50 - Training loss: 2.21938 - Validation loss: 2.23471\n",
      "Epoch 51 - Training loss: 2.21862 - Validation loss: 2.23348\n",
      "Epoch 52 - Training loss: 2.21783 - Validation loss: 2.23285\n",
      "Epoch 53 - Training loss: 2.21702 - Validation loss: 2.23233\n",
      "Epoch 54 - Training loss: 2.21628 - Validation loss: 2.23090\n",
      "Epoch 55 - Training loss: 2.21559 - Validation loss: 2.22991\n",
      "Epoch 56 - Training loss: 2.21483 - Validation loss: 2.22933\n",
      "Epoch 57 - Training loss: 2.21413 - Validation loss: 2.22945\n",
      "Epoch 58 - Training loss: 2.21339 - Validation loss: 2.22857\n",
      "Epoch 59 - Training loss: 2.21258 - Validation loss: 2.22819\n",
      "Epoch 60 - Training loss: 2.21195 - Validation loss: 2.22731\n",
      "Epoch 61 - Training loss: 2.21127 - Validation loss: 2.22674\n",
      "Epoch 62 - Training loss: 2.21074 - Validation loss: 2.22667\n",
      "Epoch 63 - Training loss: 2.21008 - Validation loss: 2.22615\n",
      "Epoch 64 - Training loss: 2.20941 - Validation loss: 2.22555\n",
      "Epoch 65 - Training loss: 2.20876 - Validation loss: 2.22511\n",
      "Epoch 66 - Training loss: 2.20806 - Validation loss: 2.22423\n",
      "Epoch 67 - Training loss: 2.20745 - Validation loss: 2.22458\n",
      "Epoch 68 - Training loss: 2.20678 - Validation loss: 2.22398\n",
      "Epoch 69 - Training loss: 2.20623 - Validation loss: 2.22321\n",
      "Epoch 70 - Training loss: 2.20558 - Validation loss: 2.22368\n",
      "Epoch 71 - Training loss: 2.20497 - Validation loss: 2.22193\n",
      "Epoch 72 - Training loss: 2.20437 - Validation loss: 2.22167\n",
      "Epoch 73 - Training loss: 2.20382 - Validation loss: 2.22122\n",
      "Epoch 74 - Training loss: 2.20324 - Validation loss: 2.22060\n",
      "Epoch 75 - Training loss: 2.20262 - Validation loss: 2.22017\n",
      "Epoch 76 - Training loss: 2.20194 - Validation loss: 2.21947\n",
      "Epoch 77 - Training loss: 2.20138 - Validation loss: 2.21841\n",
      "Epoch 78 - Training loss: 2.20079 - Validation loss: 2.21775\n",
      "Epoch 79 - Training loss: 2.20023 - Validation loss: 2.21718\n",
      "Epoch 80 - Training loss: 2.18430 - Validation loss: 2.20436\n",
      "Epoch 81 - Training loss: 2.18235 - Validation loss: 2.20378\n",
      "Epoch 82 - Training loss: 2.18190 - Validation loss: 2.20356\n",
      "Epoch 83 - Training loss: 2.18164 - Validation loss: 2.20335\n",
      "Epoch 84 - Training loss: 2.18143 - Validation loss: 2.20316\n",
      "Epoch 85 - Training loss: 2.18125 - Validation loss: 2.20309\n",
      "Epoch 86 - Training loss: 2.18110 - Validation loss: 2.20298\n",
      "Epoch 87 - Training loss: 2.18096 - Validation loss: 2.20292\n",
      "Epoch 88 - Training loss: 2.18083 - Validation loss: 2.20280\n",
      "Epoch 89 - Training loss: 2.18072 - Validation loss: 2.20271\n",
      "Epoch 90 - Training loss: 2.18060 - Validation loss: 2.20263\n",
      "Epoch 91 - Training loss: 2.18048 - Validation loss: 2.20256\n",
      "Epoch 92 - Training loss: 2.18038 - Validation loss: 2.20250\n",
      "Epoch 93 - Training loss: 2.18027 - Validation loss: 2.20245\n",
      "Epoch 94 - Training loss: 2.18017 - Validation loss: 2.20235\n",
      "Epoch 95 - Training loss: 2.18008 - Validation loss: 2.20228\n",
      "Epoch 96 - Training loss: 2.17998 - Validation loss: 2.20222\n",
      "Epoch 97 - Training loss: 2.17988 - Validation loss: 2.20214\n",
      "Epoch 98 - Training loss: 2.17979 - Validation loss: 2.20209\n",
      "Epoch 99 - Training loss: 2.17970 - Validation loss: 2.20200\n",
      "Epoch 100 - Training loss: 2.17960 - Validation loss: 2.20193\n",
      "Epoch 101 - Training loss: 2.17951 - Validation loss: 2.20188\n",
      "Epoch 102 - Training loss: 2.17942 - Validation loss: 2.20182\n",
      "Epoch 103 - Training loss: 2.17934 - Validation loss: 2.20175\n",
      "Epoch 104 - Training loss: 2.17926 - Validation loss: 2.20171\n",
      "Epoch 105 - Training loss: 2.17918 - Validation loss: 2.20163\n",
      "Epoch 106 - Training loss: 2.17910 - Validation loss: 2.20160\n",
      "Epoch 107 - Training loss: 2.17901 - Validation loss: 2.20153\n",
      "Epoch 108 - Training loss: 2.17892 - Validation loss: 2.20148\n",
      "Epoch 109 - Training loss: 2.17885 - Validation loss: 2.20143\n",
      "Epoch 110 - Training loss: 2.17877 - Validation loss: 2.20135\n",
      "Epoch 111 - Training loss: 2.17870 - Validation loss: 2.20129\n",
      "Epoch 112 - Training loss: 2.17862 - Validation loss: 2.20122\n",
      "Epoch 113 - Training loss: 2.17854 - Validation loss: 2.20117\n",
      "Epoch 114 - Training loss: 2.17847 - Validation loss: 2.20113\n",
      "Epoch 115 - Training loss: 2.17840 - Validation loss: 2.20108\n",
      "Epoch 116 - Training loss: 2.17833 - Validation loss: 2.20101\n",
      "Epoch 117 - Training loss: 2.17826 - Validation loss: 2.20096\n",
      "Epoch 118 - Training loss: 2.17820 - Validation loss: 2.20092\n",
      "Epoch 119 - Training loss: 2.17813 - Validation loss: 2.20086\n",
      "Epoch 120 - Training loss: 2.17806 - Validation loss: 2.20079\n",
      "Epoch 121 - Training loss: 2.17799 - Validation loss: 2.20074\n",
      "Epoch 122 - Training loss: 2.17793 - Validation loss: 2.20069\n",
      "Epoch 123 - Training loss: 2.17786 - Validation loss: 2.20066\n",
      "Epoch 124 - Training loss: 2.17780 - Validation loss: 2.20064\n",
      "Epoch 125 - Training loss: 2.17774 - Validation loss: 2.20058\n",
      "Epoch 126 - Training loss: 2.17769 - Validation loss: 2.20055\n",
      "Epoch 127 - Training loss: 2.17763 - Validation loss: 2.20051\n",
      "Epoch 128 - Training loss: 2.17757 - Validation loss: 2.20048\n",
      "Epoch 129 - Training loss: 2.17751 - Validation loss: 2.20042\n",
      "Epoch 130 - Training loss: 2.17745 - Validation loss: 2.20034\n",
      "Epoch 131 - Training loss: 2.17739 - Validation loss: 2.20028\n",
      "Epoch 132 - Training loss: 2.17733 - Validation loss: 2.20023\n",
      "Epoch 133 - Training loss: 2.17727 - Validation loss: 2.20021\n",
      "Epoch 134 - Training loss: 2.17721 - Validation loss: 2.20016\n",
      "Epoch 135 - Training loss: 2.17715 - Validation loss: 2.20013\n",
      "Epoch 136 - Training loss: 2.17709 - Validation loss: 2.20008\n",
      "Epoch 137 - Training loss: 2.17704 - Validation loss: 2.20005\n",
      "Epoch 138 - Training loss: 2.17698 - Validation loss: 2.20002\n",
      "Epoch 139 - Training loss: 2.17692 - Validation loss: 2.19993\n",
      "Epoch 140 - Training loss: 2.17687 - Validation loss: 2.19992\n",
      "Epoch 141 - Training loss: 2.17682 - Validation loss: 2.19986\n",
      "Epoch 142 - Training loss: 2.17676 - Validation loss: 2.19979\n",
      "Epoch 143 - Training loss: 2.17671 - Validation loss: 2.19974\n",
      "Epoch 144 - Training loss: 2.17666 - Validation loss: 2.19965\n",
      "Epoch 145 - Training loss: 2.17661 - Validation loss: 2.19960\n",
      "Epoch 146 - Training loss: 2.17655 - Validation loss: 2.19958\n",
      "Epoch 147 - Training loss: 2.17650 - Validation loss: 2.19952\n",
      "Epoch 148 - Training loss: 2.17644 - Validation loss: 2.19948\n",
      "Epoch 149 - Training loss: 2.17640 - Validation loss: 2.19940\n",
      "Epoch 150 - Training loss: 2.17635 - Validation loss: 2.19938\n",
      "Epoch 151 - Training loss: 2.17630 - Validation loss: 2.19932\n",
      "Epoch 152 - Training loss: 2.17625 - Validation loss: 2.19931\n",
      "Epoch 153 - Training loss: 2.17621 - Validation loss: 2.19926\n",
      "Epoch 154 - Training loss: 2.17616 - Validation loss: 2.19924\n",
      "Epoch 155 - Training loss: 2.17610 - Validation loss: 2.19923\n",
      "Epoch 156 - Training loss: 2.17606 - Validation loss: 2.19918\n",
      "Epoch 157 - Training loss: 2.17601 - Validation loss: 2.19914\n",
      "Epoch 158 - Training loss: 2.17596 - Validation loss: 2.19908\n",
      "Epoch 159 - Training loss: 2.17591 - Validation loss: 2.19907\n",
      "Epoch 160 - Training loss: 2.17587 - Validation loss: 2.19905\n",
      "Epoch 161 - Training loss: 2.17582 - Validation loss: 2.19902\n",
      "Epoch 162 - Training loss: 2.17576 - Validation loss: 2.19900\n",
      "Epoch 163 - Training loss: 2.17572 - Validation loss: 2.19898\n",
      "Epoch 164 - Training loss: 2.17567 - Validation loss: 2.19894\n",
      "Epoch 165 - Training loss: 2.17562 - Validation loss: 2.19894\n",
      "Epoch 166 - Training loss: 2.17558 - Validation loss: 2.19890\n",
      "Epoch 167 - Training loss: 2.17552 - Validation loss: 2.19886\n",
      "Epoch 168 - Training loss: 2.17547 - Validation loss: 2.19883\n",
      "Epoch 169 - Training loss: 2.17542 - Validation loss: 2.19879\n",
      "Epoch 170 - Training loss: 2.17537 - Validation loss: 2.19877\n",
      "Epoch 171 - Training loss: 2.17532 - Validation loss: 2.19877\n",
      "Epoch 172 - Training loss: 2.17528 - Validation loss: 2.19875\n",
      "Epoch 173 - Training loss: 2.17522 - Validation loss: 2.19872\n",
      "Epoch 174 - Training loss: 2.17518 - Validation loss: 2.19871\n",
      "Epoch 175 - Training loss: 2.17514 - Validation loss: 2.19868\n",
      "Epoch 176 - Training loss: 2.17510 - Validation loss: 2.19865\n",
      "Epoch 177 - Training loss: 2.17505 - Validation loss: 2.19862\n",
      "Epoch 178 - Training loss: 2.17501 - Validation loss: 2.19857\n",
      "Epoch 179 - Training loss: 2.17497 - Validation loss: 2.19857\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "epochs = 80+100\n",
    "lrs = [0.1, 0.01]\n",
    "batch_size = 64\n",
    "\n",
    "for i in range(epochs):\n",
    "    train_loss = []\n",
    "    if i < 80:\n",
    "        lr = lrs[0]\n",
    "    else:\n",
    "        lr = lrs[1]\n",
    "    for b in range(0, len(X_train), batch_size):\n",
    "        # Model\n",
    "        X_batch = X_train[b:b+batch_size]\n",
    "        y_batch = y_train[b:b+batch_size]\n",
    "        # print(X_batch.shape, y_batch.shape)\n",
    "       \n",
    "        emb_layer = E[X_batch].view(-1, context * emb_size) # embed each token and concatenate the context tokens\n",
    "        # print(E.shape)\n",
    "\n",
    "        dense_layer = ((emb_layer @ W1) + b1).relu()\n",
    "        dense_layer = (dense_layer @ W2) + b2\n",
    "        # logits = dense_layer.exp()\n",
    "        # norm = logits.sum(1, keepdim=True)\n",
    "        # probs = logits / norm\n",
    "\n",
    "        # Loss\n",
    "        loss = F.cross_entropy(dense_layer, y_batch)\n",
    "        # loss = -probs[[torch.arange(len(y_batch)), torch.tensor(y_batch)]].log().mean()\n",
    "\n",
    "        # print(loss.item())\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        # Update\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        for p in parameters:\n",
    "            p.data -= lr * p.grad\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        emb_layer = E[X_valid].view(-1, context * emb_size) # embed each token and concatenate the context tokens\n",
    "        dense_layer = ((emb_layer @ W1) + b1).relu()\n",
    "        dense_layer = (dense_layer @ W2) + b2\n",
    "        valid_loss = F.cross_entropy(dense_layer, y_valid)\n",
    "\n",
    "    print(f\"Epoch {i} - Training loss: {sum(train_loss)/len(train_loss):.5f} - Validation loss: {valid_loss:.5f}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1681)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    emb_layer = E[X_test].view(-1, context * emb_size) # embed each token and concatenate the context tokens\n",
    "    dense_layer = ((emb_layer @ W1) + b1).relu()\n",
    "    dense_layer = (dense_layer @ W2) + b2\n",
    "    test_loss = F.cross_entropy(dense_layer, y_test)\n",
    "\n",
    "test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....alewy.\n",
      "....roirynn.\n",
      "....jaigh.\n",
      "....zohy.\n",
      "....nekenabrikyn.\n",
      "....jai.\n",
      "....shil.\n",
      "....crecar.\n",
      "....leor.\n",
      "....avisa.\n"
     ]
    }
   ],
   "source": [
    "names_to_generate = 5\n",
    "\n",
    "def generate_name():\n",
    "    inp = torch.tensor([[stoix[S_TOK]]*context])\n",
    "    # print(inp)\n",
    "    # print(inp[:,-4:])\n",
    "    while True:\n",
    "        # print(inp.shape)\n",
    "        # print(E[inp].shape)\n",
    "        emb_layer = E[inp[:,-4:]].view(-1, context * emb_size) # embed each token and concatenate the context tokens\n",
    "        dense_layer = ((emb_layer @ W1) + b1).relu()\n",
    "        dense_layer = (dense_layer @ W2) + b2\n",
    "        prob = dense_layer.exp()\n",
    "        prob /= prob.sum()\n",
    "        # next_token = dense_layer.argmax(1, keepdim=True)\n",
    "        next_token = torch.multinomial(prob, 1)\n",
    "        # print(next_token.item())\n",
    "        # print(torch.cat((inp, next_token), 1))\n",
    "        inp = torch.cat((inp, next_token), 1)\n",
    "        if next_token[0].item() == stoix[S_TOK]:\n",
    "            break\n",
    "    \n",
    "    return ''.join([ixtos[i] for i in inp[0]])\n",
    "    \n",
    "    # test_loss = F.cross_entropy(dense_layer, y_test)\n",
    "\n",
    "for i in range(10):\n",
    "    print(generate_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
